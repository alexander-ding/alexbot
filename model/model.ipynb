{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the good stuff\n",
    "conversation_dict = np.load(Path(\"data\") / \"conversation_dict.npy\").item()\n",
    "with open(Path(\"data\") / \"word_list.txt\", \"rb\") as f:\n",
    "    word_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants here\n",
    "max_length = 20\n",
    "batch_size = 32\n",
    "vocab_size = len(word_list)\n",
    "embedding_dim = 256\n",
    "hidden_units = 1024\n",
    "attention_units = 16\n",
    "epochs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train():\n",
    "    # make x_train and y_train\n",
    "\n",
    "    # setup empty ones to be filled\n",
    "    dataset_size = len(conversation_dict)\n",
    "    x_train = np.zeros((dataset_size, max_length), dtype='int32')\n",
    "    y_train = np.zeros((dataset_size, max_length), dtype='int32')\n",
    "\n",
    "    for index,(k,v) in enumerate(conversation_dict.items()):\n",
    "        # dummy arrays to be filled\n",
    "        in_msg = np.full((max_length), word_list.index('<padding>'), dtype='int32')\n",
    "        out_msg = np.full((max_length), word_list.index('<padding>'), dtype='int32')\n",
    "\n",
    "        # split up the words\n",
    "        in_words = k.split()[:max_length-1]\n",
    "        out_words = v.split()[:max_length-1]\n",
    "\n",
    "        # throw out empty ones\n",
    "        if (len(in_words) == 0 or len(out_words) == 0):\n",
    "            continue\n",
    "\n",
    "        # integerize the strings\n",
    "        for i, word in enumerate(in_words):\n",
    "            in_msg[i] = word_list.index(word)\n",
    "        in_msg[i+1] = word_list.index(\"<eos>\")\n",
    "\n",
    "        for i, word in enumerate(out_words):\n",
    "            out_msg[i] = word_list.index(word)\n",
    "        out_msg[i+1] = word_list.index(\"<eos>\")\n",
    "\n",
    "        x_train[index] = in_msg\n",
    "        y_train[index] = out_msg\n",
    "\n",
    "    # remove completely 0's lines\n",
    "    x_train = x_train[~np.all(x_train == 0, axis=1)]\n",
    "    y_train = y_train[~np.all(y_train == 0, axis=1)]\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data\") / \"train.npz\"\n",
    "if data_path.exists():\n",
    "    with open(data_path, \"rb\") as f:\n",
    "        archive = np.load(f)\n",
    "        x_train, y_train = archive[\"x_train\"], archive[\"y_train\"]\n",
    "else: \n",
    "    x_train, y_train = make_train()\n",
    "    np.savez(data_path, x_train=x_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train))\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, hidden_units, batch_size)\n",
    "decoder = Decoder(vocab_size, embedding_dim, hidden_units, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam()\n",
    "loss_object = lambda x, y: tf.keras.losses.sparse_categorical_crossentropy(x, y, \n",
    "                                                                           from_logits=True)\n",
    "def loss_function(real, pred):\n",
    "    # ignore 0's\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2357ff9e860>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(checkpoint_dir + \"/ckpt-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([word_list.index(\"<padding>\")] * batch_size, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 9.0677\n",
      "Epoch 1 Batch 100 Loss 3.6531\n",
      "Epoch 1 Batch 200 Loss 3.0089\n",
      "Epoch 1 Batch 300 Loss 2.4625\n",
      "Epoch 1 Batch 400 Loss 3.1726\n",
      "Epoch 1 Loss 3.1939\n",
      "Time taken for 1 epoch 157.89947128295898 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.2448\n",
      "Epoch 2 Batch 100 Loss 3.2879\n",
      "Epoch 2 Batch 200 Loss 2.7652\n",
      "Epoch 2 Batch 300 Loss 2.2220\n",
      "Epoch 2 Batch 400 Loss 2.8885\n",
      "Epoch 2 Loss 2.7675\n",
      "Time taken for 1 epoch 112.83490824699402 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.0304\n",
      "Epoch 3 Batch 100 Loss 3.0578\n",
      "Epoch 3 Batch 200 Loss 2.6307\n",
      "Epoch 3 Batch 300 Loss 2.0918\n",
      "Epoch 3 Batch 400 Loss 2.7009\n",
      "Epoch 3 Loss 2.6155\n",
      "Time taken for 1 epoch 109.4363489151001 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.9882\n",
      "Epoch 4 Batch 100 Loss 2.9279\n",
      "Epoch 4 Batch 200 Loss 2.5166\n",
      "Epoch 4 Batch 300 Loss 1.9824\n",
      "Epoch 4 Batch 400 Loss 2.5381\n",
      "Epoch 4 Loss 2.4909\n",
      "Time taken for 1 epoch 123.34249019622803 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.7588\n",
      "Epoch 5 Batch 100 Loss 2.7657\n",
      "Epoch 5 Batch 200 Loss 2.3912\n",
      "Epoch 5 Batch 300 Loss 1.8846\n",
      "Epoch 5 Batch 400 Loss 2.3782\n",
      "Epoch 5 Loss 2.3610\n",
      "Time taken for 1 epoch 114.31206822395325 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.6078\n",
      "Epoch 6 Batch 100 Loss 2.6295\n",
      "Epoch 6 Batch 200 Loss 2.2709\n",
      "Epoch 6 Batch 300 Loss 1.8072\n",
      "Epoch 6 Batch 400 Loss 2.2264\n",
      "Epoch 6 Loss 2.2387\n",
      "Time taken for 1 epoch 112.5884256362915 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.4474\n",
      "Epoch 7 Batch 100 Loss 2.4892\n",
      "Epoch 7 Batch 200 Loss 2.1446\n",
      "Epoch 7 Batch 300 Loss 1.7097\n",
      "Epoch 7 Batch 400 Loss 2.0673\n",
      "Epoch 7 Loss 2.1175\n",
      "Time taken for 1 epoch 114.17796754837036 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.2894\n",
      "Epoch 8 Batch 100 Loss 2.3646\n",
      "Epoch 8 Batch 200 Loss 2.0207\n",
      "Epoch 8 Batch 300 Loss 1.6151\n",
      "Epoch 8 Batch 400 Loss 1.9505\n",
      "Epoch 8 Loss 2.0106\n",
      "Time taken for 1 epoch 113.40239429473877 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.1530\n",
      "Epoch 9 Batch 100 Loss 2.2869\n",
      "Epoch 9 Batch 200 Loss 1.8965\n",
      "Epoch 9 Batch 300 Loss 1.5254\n",
      "Epoch 9 Batch 400 Loss 1.8592\n",
      "Epoch 9 Loss 1.9156\n",
      "Time taken for 1 epoch 112.9152524471283 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 2.0784\n",
      "Epoch 10 Batch 100 Loss 2.1352\n",
      "Epoch 10 Batch 200 Loss 1.9116\n",
      "Epoch 10 Batch 300 Loss 1.4854\n",
      "Epoch 10 Batch 400 Loss 1.8154\n",
      "Epoch 10 Loss 1.8386\n",
      "Time taken for 1 epoch 116.07594680786133 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.9628\n",
      "Epoch 11 Batch 100 Loss 2.0650\n",
      "Epoch 11 Batch 200 Loss 1.8024\n",
      "Epoch 11 Batch 300 Loss 1.4434\n",
      "Epoch 11 Batch 400 Loss 1.7066\n",
      "Epoch 11 Loss 1.7879\n",
      "Time taken for 1 epoch 110.65960335731506 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.8603\n",
      "Epoch 12 Batch 100 Loss 2.0262\n",
      "Epoch 12 Batch 200 Loss 1.7155\n",
      "Epoch 12 Batch 300 Loss 1.3538\n",
      "Epoch 12 Batch 400 Loss 1.6145\n",
      "Epoch 12 Loss 1.7043\n",
      "Time taken for 1 epoch 113.2457320690155 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.7676\n",
      "Epoch 13 Batch 100 Loss 1.9309\n",
      "Epoch 13 Batch 200 Loss 1.6223\n",
      "Epoch 13 Batch 300 Loss 1.2901\n",
      "Epoch 13 Batch 400 Loss 1.5398\n",
      "Epoch 13 Loss 1.6169\n",
      "Time taken for 1 epoch 108.94050097465515 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.7423\n",
      "Epoch 14 Batch 100 Loss 2.0085\n",
      "Epoch 14 Batch 200 Loss 1.5476\n",
      "Epoch 14 Batch 300 Loss 1.2358\n",
      "Epoch 14 Batch 400 Loss 1.4269\n",
      "Epoch 14 Loss 1.5500\n",
      "Time taken for 1 epoch 111.71820759773254 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.6381\n",
      "Epoch 15 Batch 100 Loss 1.8213\n",
      "Epoch 15 Batch 200 Loss 1.4930\n",
      "Epoch 15 Batch 300 Loss 1.1417\n",
      "Epoch 15 Batch 400 Loss 1.3344\n",
      "Epoch 15 Loss 1.4607\n",
      "Time taken for 1 epoch 108.6593108177185 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.5000\n",
      "Epoch 16 Batch 100 Loss 1.8955\n",
      "Epoch 16 Batch 200 Loss 1.3840\n",
      "Epoch 16 Batch 300 Loss 1.0791\n",
      "Epoch 16 Batch 400 Loss 1.2459\n",
      "Epoch 16 Loss 1.3779\n",
      "Time taken for 1 epoch 113.25765538215637 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.3365\n",
      "Epoch 17 Batch 100 Loss 1.6428\n",
      "Epoch 17 Batch 200 Loss 1.2692\n",
      "Epoch 17 Batch 300 Loss 1.0125\n",
      "Epoch 17 Batch 400 Loss 1.2204\n",
      "Epoch 17 Loss 1.2932\n",
      "Time taken for 1 epoch 112.01162338256836 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.3460\n",
      "Epoch 18 Batch 100 Loss 1.6824\n",
      "Epoch 18 Batch 200 Loss 1.3246\n",
      "Epoch 18 Batch 300 Loss 1.0810\n",
      "Epoch 18 Batch 400 Loss 1.2412\n",
      "Epoch 18 Loss 1.3355\n",
      "Time taken for 1 epoch 113.36594772338867 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.2919\n",
      "Epoch 19 Batch 100 Loss 1.5384\n",
      "Epoch 19 Batch 200 Loss 1.3008\n",
      "Epoch 19 Batch 300 Loss 1.0153\n",
      "Epoch 19 Batch 400 Loss 1.1642\n",
      "Epoch 19 Loss 1.2453\n",
      "Time taken for 1 epoch 112.53266859054565 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.1840\n",
      "Epoch 20 Batch 100 Loss 1.4377\n",
      "Epoch 20 Batch 200 Loss 1.1428\n",
      "Epoch 20 Batch 300 Loss 0.9291\n",
      "Epoch 20 Batch 400 Loss 1.0378\n",
      "Epoch 20 Loss 1.1641\n",
      "Time taken for 1 epoch 112.57032656669617 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 1.2486\n",
      "Epoch 21 Batch 100 Loss 1.2710\n",
      "Epoch 21 Batch 200 Loss 1.0771\n",
      "Epoch 21 Batch 300 Loss 0.9202\n",
      "Epoch 21 Batch 400 Loss 1.0062\n",
      "Epoch 21 Loss 1.1361\n",
      "Time taken for 1 epoch 112.84990406036377 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 1.1063\n",
      "Epoch 22 Batch 100 Loss 1.1613\n",
      "Epoch 22 Batch 200 Loss 1.0599\n",
      "Epoch 22 Batch 300 Loss 0.8741\n",
      "Epoch 22 Batch 400 Loss 0.9086\n",
      "Epoch 22 Loss 1.0356\n",
      "Time taken for 1 epoch 113.02527213096619 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 1.0663\n",
      "Epoch 23 Batch 100 Loss 1.1001\n",
      "Epoch 23 Batch 200 Loss 0.9218\n",
      "Epoch 23 Batch 300 Loss 0.8295\n",
      "Epoch 23 Batch 400 Loss 0.9180\n",
      "Epoch 23 Loss 0.9862\n",
      "Time taken for 1 epoch 109.17954516410828 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.9437\n",
      "Epoch 24 Batch 100 Loss 1.0845\n",
      "Epoch 24 Batch 200 Loss 0.8426\n",
      "Epoch 24 Batch 300 Loss 0.7558\n",
      "Epoch 24 Batch 400 Loss 0.8618\n",
      "Epoch 24 Loss 0.9086\n",
      "Time taken for 1 epoch 114.32942748069763 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.8351\n",
      "Epoch 25 Batch 100 Loss 0.9250\n",
      "Epoch 25 Batch 200 Loss 0.7361\n",
      "Epoch 25 Batch 300 Loss 0.7068\n",
      "Epoch 25 Batch 400 Loss 0.7435\n",
      "Epoch 25 Loss 0.8255\n",
      "Time taken for 1 epoch 110.36156439781189 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.7515\n",
      "Epoch 26 Batch 100 Loss 0.8198\n",
      "Epoch 26 Batch 200 Loss 0.6695\n",
      "Epoch 26 Batch 300 Loss 0.7003\n",
      "Epoch 26 Batch 400 Loss 0.7744\n",
      "Epoch 26 Loss 0.7690\n",
      "Time taken for 1 epoch 114.77567291259766 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.7360\n",
      "Epoch 27 Batch 100 Loss 0.7953\n",
      "Epoch 27 Batch 200 Loss 0.6093\n",
      "Epoch 27 Batch 300 Loss 0.7234\n",
      "Epoch 27 Batch 400 Loss 0.6465\n",
      "Epoch 27 Loss 0.7275\n",
      "Time taken for 1 epoch 110.77038025856018 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.6543\n",
      "Epoch 28 Batch 100 Loss 0.6619\n",
      "Epoch 28 Batch 200 Loss 0.5992\n",
      "Epoch 28 Batch 300 Loss 0.6315\n",
      "Epoch 28 Batch 400 Loss 0.5724\n",
      "Epoch 28 Loss 0.6553\n",
      "Time taken for 1 epoch 115.46283006668091 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.6803\n",
      "Epoch 29 Batch 100 Loss 0.5821\n",
      "Epoch 29 Batch 200 Loss 0.5019\n",
      "Epoch 29 Batch 300 Loss 0.5842\n",
      "Epoch 29 Batch 400 Loss 0.5166\n",
      "Epoch 29 Loss 0.5893\n",
      "Time taken for 1 epoch 109.59194207191467 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.5841\n",
      "Epoch 30 Batch 100 Loss 0.5821\n",
      "Epoch 30 Batch 200 Loss 0.5160\n",
      "Epoch 30 Batch 300 Loss 0.5307\n",
      "Epoch 30 Batch 400 Loss 0.4882\n",
      "Epoch 30 Loss 0.5624\n",
      "Time taken for 1 epoch 114.07896256446838 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.5179\n",
      "Epoch 31 Batch 100 Loss 0.5188\n",
      "Epoch 31 Batch 200 Loss 0.4412\n",
      "Epoch 31 Batch 300 Loss 0.4842\n",
      "Epoch 31 Batch 400 Loss 0.4076\n",
      "Epoch 31 Loss 0.4922\n",
      "Time taken for 1 epoch 111.07500886917114 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.4482\n",
      "Epoch 32 Batch 100 Loss 0.4034\n",
      "Epoch 32 Batch 200 Loss 0.3752\n",
      "Epoch 32 Batch 300 Loss 0.4043\n",
      "Epoch 32 Batch 400 Loss 0.4491\n",
      "Epoch 32 Loss 0.4378\n",
      "Time taken for 1 epoch 113.94091582298279 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(x_train) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(msg):\n",
    "    msg = msg.replace('\\n', ' ').lower()\n",
    "    msg = msg.replace(\"\\xc2\\xa0\", \"\")\n",
    "    msg = re.sub('([\\(\\).,!?])', \"\", msg)\n",
    "    msg = re.sub(\" +\",\" \", msg)\n",
    "    return msg\n",
    "\n",
    "def softmax_choose(a, weights):\n",
    "    exps = np.exp(weights - np.max(weights))\n",
    "    scaled_exps = exps / np.sum(exps)\n",
    "    return np.random.choice(a, p=scaled_exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bot:\n",
    "    def __init__(self, word_list, encoder, decoder):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.word_list = word_list\n",
    "        \n",
    "    def encode(self, sentence):\n",
    "        inputs = [self.word_list.index(i) for i in sentence.split(' ') if i in self.word_list]\n",
    "        inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                               maxlen=max_length,\n",
    "                                                               padding='post')\n",
    "        inputs = tf.convert_to_tensor(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        result = []\n",
    "\n",
    "        hidden = [tf.zeros((1, hidden_units))]\n",
    "        enc_out, enc_hidden = self.encoder(inputs, hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([self.word_list.index('<padding>')], 0)\n",
    "\n",
    "        for t in range(max_length):\n",
    "            predictions, dec_hidden, attention_weights = self.decoder(dec_input,\n",
    "                                                                 dec_hidden,\n",
    "                                                                 enc_out)\n",
    "            options = 8 if t == 0 else 1\n",
    "            top_prediction_indices = tf.argsort(predictions[0])[::-1][:options]\n",
    "            top_confidences = []\n",
    "            for index in top_prediction_indices:\n",
    "                top_confidences.append(float(predictions[0][index]))\n",
    "            top_confidences = np.array(top_confidences)\n",
    "            predicted_id = softmax_choose(top_prediction_indices, top_confidences)\n",
    "\n",
    "            result.append(predicted_id)\n",
    "            if self.word_list[predicted_id] == '<eos>':\n",
    "                return result\n",
    "\n",
    "            # the predicted ID is fed back into the model\n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def decode(self, outputs):\n",
    "        return \" \".join([self.word_list[w] for w in outputs if self.word_list[w] != \"<eos>\" and self.word_list[w] != \"<padding>\"])\n",
    "\n",
    "    def evaluate(self, sentence):\n",
    "        sentence = preprocess_sentence(sentence)\n",
    "        inputs = self.encode(sentence)\n",
    "        outputs = self.predict(inputs)\n",
    "        response = self.decode(outputs)\n",
    "\n",
    "        return sentence, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexBot = Bot(word_list, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hi', 'i love you ❤️')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AlexBot.evaluate(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i got those later and sure sounds great\n",
      "aw ok we’re really having a hard time spending time together tho aren’t we\n"
     ]
    }
   ],
   "source": [
    "i = 304\n",
    "print(decode(x_train[i]))\n",
    "print(decode(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "Here is the reversed Alex Botexperiment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the good stuff\n",
    "conversation_dict = np.load(Path(\"data\") / \"conversation_dict_reverse.npy\").item()\n",
    "with open(Path(\"data\") / \"word_list_reverse.txt\", \"rb\") as f:\n",
    "    word_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train():\n",
    "    # make x_train and y_train\n",
    "\n",
    "    # setup empty ones to be filled\n",
    "    dataset_size = len(conversation_dict)\n",
    "    x_train = np.zeros((dataset_size, max_length), dtype='int32')\n",
    "    y_train = np.zeros((dataset_size, max_length), dtype='int32')\n",
    "\n",
    "    for index,(k,v) in enumerate(conversation_dict.items()):\n",
    "        # dummy arrays to be filled\n",
    "        in_msg = np.full((max_length), word_list.index('<padding>'), dtype='int32')\n",
    "        out_msg = np.full((max_length), word_list.index('<padding>'), dtype='int32')\n",
    "\n",
    "        # split up the words\n",
    "        in_words = k.split()[:max_length-1]\n",
    "        out_words = v.split()[:max_length-1]\n",
    "\n",
    "        # throw out empty ones\n",
    "        if (len(in_words) == 0 or len(out_words) == 0):\n",
    "            continue\n",
    "\n",
    "        # integerize the strings\n",
    "        for i, word in enumerate(in_words):\n",
    "            in_msg[i] = word_list.index(word)\n",
    "        in_msg[i+1] = word_list.index(\"<eos>\")\n",
    "\n",
    "        for i, word in enumerate(out_words):\n",
    "            out_msg[i] = word_list.index(word)\n",
    "        out_msg[i+1] = word_list.index(\"<eos>\")\n",
    "\n",
    "        x_train[index] = in_msg\n",
    "        y_train[index] = out_msg\n",
    "\n",
    "    # remove completely 0's lines\n",
    "    x_train = x_train[~np.all(x_train == 0, axis=1)]\n",
    "    y_train = y_train[~np.all(y_train == 0, axis=1)]\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data\") / \"train_reverse.npz\"\n",
    "if data_path.exists():\n",
    "    with open(data_path, \"rb\") as f:\n",
    "        archive = np.load(f)\n",
    "        x_train, y_train = archive[\"x_train\"], archive[\"y_train\"]\n",
    "else: \n",
    "    x_train, y_train = make_train()\n",
    "    np.savez(data_path, x_train=x_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train))\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "vocab_size = len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, hidden_units, batch_size)\n",
    "decoder = Decoder(vocab_size, embedding_dim, hidden_units, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam()\n",
    "loss_object = lambda x, y: tf.keras.losses.sparse_categorical_crossentropy(x, y, \n",
    "                                                                           from_logits=True)\n",
    "def loss_function(real, pred):\n",
    "    # ignore 0's\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './reverse_training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2355e75c9e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(checkpoint_dir + \"/ckpt-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([word_list.index(\"<padding>\")] * batch_size, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.4804\n",
      "Epoch 1 Batch 100 Loss 2.9065\n",
      "Epoch 1 Batch 200 Loss 3.1111\n",
      "Epoch 1 Batch 300 Loss 2.5116\n",
      "Epoch 1 Batch 400 Loss 2.6553\n",
      "Epoch 1 Loss 2.8190\n",
      "Time taken for 1 epoch 113.67062044143677 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.4899\n",
      "Epoch 2 Batch 100 Loss 2.6109\n",
      "Epoch 2 Batch 200 Loss 2.8259\n",
      "Epoch 2 Batch 300 Loss 2.2920\n",
      "Epoch 2 Batch 400 Loss 2.4332\n",
      "Epoch 2 Loss 2.5038\n",
      "Time taken for 1 epoch 117.27589726448059 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.2936\n",
      "Epoch 3 Batch 100 Loss 2.4329\n",
      "Epoch 3 Batch 200 Loss 2.6602\n",
      "Epoch 3 Batch 300 Loss 2.2026\n",
      "Epoch 3 Batch 400 Loss 2.3040\n",
      "Epoch 3 Loss 2.3616\n",
      "Time taken for 1 epoch 116.63586235046387 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.1633\n",
      "Epoch 4 Batch 100 Loss 2.3184\n",
      "Epoch 4 Batch 200 Loss 2.5264\n",
      "Epoch 4 Batch 300 Loss 2.1084\n",
      "Epoch 4 Batch 400 Loss 2.1919\n",
      "Epoch 4 Loss 2.2477\n",
      "Time taken for 1 epoch 127.47735953330994 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.0394\n",
      "Epoch 5 Batch 100 Loss 2.1986\n",
      "Epoch 5 Batch 200 Loss 2.3757\n",
      "Epoch 5 Batch 300 Loss 2.0080\n",
      "Epoch 5 Batch 400 Loss 2.0896\n",
      "Epoch 5 Loss 2.1315\n",
      "Time taken for 1 epoch 133.99092888832092 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.9272\n",
      "Epoch 6 Batch 100 Loss 2.1158\n",
      "Epoch 6 Batch 200 Loss 2.2130\n",
      "Epoch 6 Batch 300 Loss 1.9041\n",
      "Epoch 6 Batch 400 Loss 2.0120\n",
      "Epoch 6 Loss 2.0282\n",
      "Time taken for 1 epoch 134.23917603492737 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.8211\n",
      "Epoch 7 Batch 100 Loss 2.0221\n",
      "Epoch 7 Batch 200 Loss 2.0972\n",
      "Epoch 7 Batch 300 Loss 1.8262\n",
      "Epoch 7 Batch 400 Loss 1.9657\n",
      "Epoch 7 Loss 1.9348\n",
      "Time taken for 1 epoch 128.86587119102478 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.7193\n",
      "Epoch 8 Batch 100 Loss 1.9372\n",
      "Epoch 8 Batch 200 Loss 1.9784\n",
      "Epoch 8 Batch 300 Loss 1.7193\n",
      "Epoch 8 Batch 400 Loss 1.8522\n",
      "Epoch 8 Loss 1.8325\n",
      "Time taken for 1 epoch 132.93056535720825 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.6452\n",
      "Epoch 9 Batch 100 Loss 1.8637\n",
      "Epoch 9 Batch 200 Loss 1.8810\n",
      "Epoch 9 Batch 300 Loss 1.6532\n",
      "Epoch 9 Batch 400 Loss 1.7915\n",
      "Epoch 9 Loss 1.7503\n",
      "Time taken for 1 epoch 128.70968341827393 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.5647\n",
      "Epoch 10 Batch 100 Loss 1.8141\n",
      "Epoch 10 Batch 200 Loss 2.0581\n",
      "Epoch 10 Batch 300 Loss 1.5916\n",
      "Epoch 10 Batch 400 Loss 1.7293\n",
      "Epoch 10 Loss 1.6952\n",
      "Time taken for 1 epoch 132.14178657531738 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.4956\n",
      "Epoch 11 Batch 100 Loss 1.6954\n",
      "Epoch 11 Batch 200 Loss 1.7294\n",
      "Epoch 11 Batch 300 Loss 1.5480\n",
      "Epoch 11 Batch 400 Loss 1.6484\n",
      "Epoch 11 Loss 1.6146\n",
      "Time taken for 1 epoch 130.73376083374023 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.4213\n",
      "Epoch 12 Batch 100 Loss 1.6306\n",
      "Epoch 12 Batch 200 Loss 1.6786\n",
      "Epoch 12 Batch 300 Loss 1.4786\n",
      "Epoch 12 Batch 400 Loss 1.6659\n",
      "Epoch 12 Loss 1.5811\n",
      "Time taken for 1 epoch 132.56056833267212 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.3864\n",
      "Epoch 13 Batch 100 Loss 1.5814\n",
      "Epoch 13 Batch 200 Loss 1.6283\n",
      "Epoch 13 Batch 300 Loss 1.4386\n",
      "Epoch 13 Batch 400 Loss 1.6070\n",
      "Epoch 13 Loss 1.5264\n",
      "Time taken for 1 epoch 129.2647659778595 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.3782\n",
      "Epoch 14 Batch 100 Loss 1.4847\n",
      "Epoch 14 Batch 200 Loss 1.4951\n",
      "Epoch 14 Batch 300 Loss 1.2994\n",
      "Epoch 14 Batch 400 Loss 1.5167\n",
      "Epoch 14 Loss 1.4615\n",
      "Time taken for 1 epoch 132.93750381469727 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.3028\n",
      "Epoch 15 Batch 100 Loss 1.4823\n",
      "Epoch 15 Batch 200 Loss 1.5519\n",
      "Epoch 15 Batch 300 Loss 1.3481\n",
      "Epoch 15 Batch 400 Loss 1.4977\n",
      "Epoch 15 Loss 1.4486\n",
      "Time taken for 1 epoch 129.9901852607727 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.2423\n",
      "Epoch 16 Batch 100 Loss 1.3900\n",
      "Epoch 16 Batch 200 Loss 1.3555\n",
      "Epoch 16 Batch 300 Loss 1.5632\n",
      "Epoch 16 Batch 400 Loss 1.4327\n",
      "Epoch 16 Loss 1.3504\n",
      "Time taken for 1 epoch 133.3923361301422 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.1940\n",
      "Epoch 17 Batch 100 Loss 1.4116\n",
      "Epoch 17 Batch 200 Loss 1.2753\n",
      "Epoch 17 Batch 300 Loss 1.1956\n",
      "Epoch 17 Batch 400 Loss 1.3448\n",
      "Epoch 17 Loss 1.2696\n",
      "Time taken for 1 epoch 131.06353616714478 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.1770\n",
      "Epoch 18 Batch 100 Loss 1.3072\n",
      "Epoch 18 Batch 200 Loss 1.3217\n",
      "Epoch 18 Batch 300 Loss 1.1576\n",
      "Epoch 18 Batch 400 Loss 1.2749\n",
      "Epoch 18 Loss 1.2299\n",
      "Time taken for 1 epoch 133.96722173690796 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.1141\n",
      "Epoch 19 Batch 100 Loss 1.1903\n",
      "Epoch 19 Batch 200 Loss 1.2842\n",
      "Epoch 19 Batch 300 Loss 1.1137\n",
      "Epoch 19 Batch 400 Loss 1.2856\n",
      "Epoch 19 Loss 1.2032\n",
      "Time taken for 1 epoch 131.6529951095581 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.0685\n",
      "Epoch 20 Batch 100 Loss 1.1702\n",
      "Epoch 20 Batch 200 Loss 1.2125\n",
      "Epoch 20 Batch 300 Loss 1.0841\n",
      "Epoch 20 Batch 400 Loss 1.2039\n",
      "Epoch 20 Loss 1.1363\n",
      "Time taken for 1 epoch 134.1089425086975 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 1.0208\n",
      "Epoch 21 Batch 100 Loss 1.1020\n",
      "Epoch 21 Batch 200 Loss 1.1307\n",
      "Epoch 21 Batch 300 Loss 1.0139\n",
      "Epoch 21 Batch 400 Loss 1.1217\n",
      "Epoch 21 Loss 1.0629\n",
      "Time taken for 1 epoch 123.95116853713989 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 1.0269\n",
      "Epoch 22 Batch 100 Loss 1.0467\n",
      "Epoch 22 Batch 200 Loss 1.0032\n",
      "Epoch 22 Batch 300 Loss 0.9288\n",
      "Epoch 22 Batch 400 Loss 1.1734\n",
      "Epoch 22 Loss 1.0171\n",
      "Time taken for 1 epoch 129.44524669647217 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.9859\n",
      "Epoch 23 Batch 100 Loss 0.9495\n",
      "Epoch 23 Batch 200 Loss 0.9955\n",
      "Epoch 23 Batch 300 Loss 0.9097\n",
      "Epoch 23 Batch 400 Loss 1.0219\n",
      "Epoch 23 Loss 0.9588\n",
      "Time taken for 1 epoch 125.14473247528076 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.9091\n",
      "Epoch 24 Batch 100 Loss 0.8898\n",
      "Epoch 24 Batch 200 Loss 0.9285\n",
      "Epoch 24 Batch 300 Loss 0.8653\n",
      "Epoch 24 Batch 400 Loss 0.9875\n",
      "Epoch 24 Loss 0.9063\n",
      "Time taken for 1 epoch 137.23850965499878 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.8550\n",
      "Epoch 25 Batch 100 Loss 0.8464\n",
      "Epoch 25 Batch 200 Loss 0.9261\n",
      "Epoch 25 Batch 300 Loss 0.9008\n",
      "Epoch 25 Batch 400 Loss 0.9692\n",
      "Epoch 25 Loss 0.8699\n",
      "Time taken for 1 epoch 132.6683428287506 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.7821\n",
      "Epoch 26 Batch 100 Loss 0.8151\n",
      "Epoch 26 Batch 200 Loss 0.8198\n",
      "Epoch 26 Batch 300 Loss 0.7410\n",
      "Epoch 26 Batch 400 Loss 0.9504\n",
      "Epoch 26 Loss 0.8248\n",
      "Time taken for 1 epoch 135.97382402420044 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.7137\n",
      "Epoch 27 Batch 100 Loss 0.7643\n",
      "Epoch 27 Batch 200 Loss 0.7288\n",
      "Epoch 27 Batch 300 Loss 0.6941\n",
      "Epoch 27 Batch 400 Loss 0.8130\n",
      "Epoch 27 Loss 0.7501\n",
      "Time taken for 1 epoch 135.26890516281128 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.6933\n",
      "Epoch 28 Batch 100 Loss 0.6990\n",
      "Epoch 28 Batch 200 Loss 0.6187\n",
      "Epoch 28 Batch 300 Loss 0.6225\n",
      "Epoch 28 Batch 400 Loss 0.6907\n",
      "Epoch 28 Loss 0.6679\n",
      "Time taken for 1 epoch 136.7487931251526 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.5910\n",
      "Epoch 29 Batch 100 Loss 0.6497\n",
      "Epoch 29 Batch 200 Loss 0.5493\n",
      "Epoch 29 Batch 300 Loss 0.5729\n",
      "Epoch 29 Batch 400 Loss 0.6904\n",
      "Epoch 29 Loss 0.6037\n",
      "Time taken for 1 epoch 132.68275833129883 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.5395\n",
      "Epoch 30 Batch 100 Loss 0.5496\n",
      "Epoch 30 Batch 200 Loss 0.5003\n",
      "Epoch 30 Batch 300 Loss 0.5593\n",
      "Epoch 30 Batch 400 Loss 0.6467\n",
      "Epoch 30 Loss 0.5510\n",
      "Time taken for 1 epoch 135.4687738418579 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.4666\n",
      "Epoch 31 Batch 100 Loss 0.5198\n",
      "Epoch 31 Batch 200 Loss 0.5501\n",
      "Epoch 31 Batch 300 Loss 0.5133\n",
      "Epoch 31 Batch 400 Loss 0.7059\n",
      "Epoch 31 Loss 0.5906\n",
      "Time taken for 1 epoch 132.81177711486816 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.4655\n",
      "Epoch 32 Batch 100 Loss 0.5066\n",
      "Epoch 32 Batch 200 Loss 0.5045\n",
      "Epoch 32 Batch 300 Loss 0.4601\n",
      "Epoch 32 Batch 400 Loss 0.5802\n",
      "Epoch 32 Loss 0.5172\n",
      "Time taken for 1 epoch 136.49701952934265 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(x_train) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "OtherBot = Bot(word_list, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('oh well drawing and painting has a teacher around',\n",
       " \"don't think i could teleport that\")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OtherBot.evaluate(\"oh well drawing and painting has a teacher around\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hi',\n",
       " 'know when he not 2 years and a lot and when he not 2 years and a lot and when')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AlexBot.evaluate(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh well drawing and painting has a teacher around\n",
      "you noticed that okay xd\n"
     ]
    }
   ],
   "source": [
    "i = 120\n",
    "print(decode(x_train[i]))\n",
    "\n",
    "print(decode(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Encoder at 0x2355e9c4d30>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
